{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593d603b-9920-4709-9075-58f4d32800bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed695f1-43a7-4c43-bdb9-6a3ece486899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 18:37:08.173960: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-02 18:37:08.174021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# tensorflow hub\n",
    "import tensorflow_hub as hub\n",
    "# tensor flow module\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# matplotlib\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# word vectorizor\n",
    "# first converts the text into a matrix of word counts\n",
    "# then transforms these counts by normalizing them based on the term frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# used to create word encoders\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad1267d-ddfd-4449-b232-2bfe363b28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/tweet_emotions.csv\")\n",
    "train_df = df[:int(df.shape[0]*0.8)]\n",
    "test_df = df[int(df.shape[0]*0.8 ):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723f7494-8566-4bf4-b3e8-0d311e83916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 18:44:25.848675: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1205726720 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\n",
    "X_train_embeddings = embed(train_df[\"content\"].values)\n",
    "X_test_embeddings = embed(test_df[\"content\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706940a7-01c9-4622-a6d4-9ba1a5942819",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_matrix = X_train_embeddings['outputs'].numpy()\n",
    "X_test_matrix = X_test_embeddings['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641794cd-7509-40b6-a4e6-0b84af5d6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.constant(train_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fdc1ebe-0739-4a8b-9f8c-cab8a4950854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNaiveBayesClassifier:\n",
    "    dist = None\n",
    "    \n",
    "    # X is the matrix containing the vectors for each sentence\n",
    "    # y is the list target values in the same order as the X matrix\n",
    "    def fit(self, X, y):\n",
    "        unique_y = np.unique(y) # unique target values: 0,1\n",
    "        print(unique_y)\n",
    "        # `points_by_class` is a numpy array the size of \n",
    "        # the number of unique targets.\n",
    "        # in each item of the list is another list that contains the vector\n",
    "        # of each sentence from the same target value\n",
    "        points_by_class = np.asarray([np.asarray(\n",
    "            [np.asarray(\n",
    "                X.iloc[x,:]) for x in range(0,len(y)) if y[x] == c]) for c in unique_y])\n",
    "        mean_list=[]\n",
    "        var_list=[]\n",
    "        for i in range(0, len(points_by_class)):\n",
    "            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n",
    "            mean_list.append(mean_var)\n",
    "            var_list.append(var_var)\n",
    "        mean=tf.stack(mean_list, 0)\n",
    "        var=tf.stack(var_list, 0)\n",
    "        # Create a 3x2 univariate normal distribution with the \n",
    "        # known mean and variance\n",
    "        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert self.dist is not None\n",
    "        nb_classes, nb_features = map(int, self.dist.scale.shape)\n",
    "\n",
    "        # uniform priors\n",
    "        priors = np.log(np.array([1. / nb_classes] * nb_classes))\n",
    "        \n",
    "        # Conditional probabilities log P(x|c)\n",
    "        # (nb_samples, nb_classes, nb_features)\n",
    "        all_log_probs = self.dist.log_prob(\n",
    "            tf.reshape(\n",
    "                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n",
    "        # (nb_samples, nb_classes)\n",
    "        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n",
    "        \n",
    "        # posterior log probability, log P(c) + log P(x|c)\n",
    "        joint_likelihood = tf.add(priors, cond_probs)\n",
    "\n",
    "        # normalize to get (log)-probabilities\n",
    "        norm_factor = tf.reduce_logsumexp(\n",
    "            joint_likelihood, axis=1, keepdims=True)\n",
    "        log_prob = joint_likelihood - norm_factor\n",
    "        # exp to get the actual probabilities\n",
    "        return tf.exp(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97561c03-bbe4-45fc-bd00-407f5b2aea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'anger' b'boredom' b'empty' b'enthusiasm' b'fun' b'happiness' b'hate'\n",
      " b'love' b'neutral' b'relief' b'sadness' b'surprise' b'worry']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_550/3513809760.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  points_by_class = np.asarray([np.asarray(\n"
     ]
    }
   ],
   "source": [
    "tf_nb = TFNaiveBayesClassifier()\n",
    "tf_nb.fit(pd.DataFrame(X_train_matrix),\n",
    "          y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18002d88-2c17-41af-8ddb-18c44363a0ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_550/1523543843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_550/3513809760.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# posterior log probability, log P(c) + log P(x|c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mjoint_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# normalize to get (log)-probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "y_pred = tf_nb.predict(X_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4eb15-f8ec-43c7-a55e-92e3a26e3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "predProbGivenText_df = pd.DataFrame(y_pred.numpy())\n",
    "predProbGivenText_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f159b-7f13-4059-a980-61bba6700b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_keywords = train_df[\"keyword\"].unique()[1:]\n",
    "print(len(uniq_keywords))\n",
    "print(uniq_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ab548-2007-482a-965e-e82e6cfe3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keywords(df_og):\n",
    "    df = df_og.copy()\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"ablaze\",\"blaze\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"blazing\",\"blaze\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"annihilated\",\"annihilation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"attacked\",\"attack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bioterror\",\"bioterrorism\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"blown%20up\",\"blew%20up\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bloody\",\"blood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bleeding\",\"blood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bags\",\"body%20bag\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bagging\",\"body%20bag\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bombed\",\"bomb\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bombing\",\"bomb\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"burning%20buildings\",\"buildings%20burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"buildings%20on%20fire\",\"buildings%20burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"burned\",\"burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"casualties\",\"casualty\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"catastrophe\",\"catastrophic\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collapse\",\"collapsed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collide\",\"collision\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collided\",\"collision\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"crash\",\"crashed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"crush\",\"crushed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"dead\",\"death\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"deaths\",\"death\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"deluge\",\"deluged\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"demolished\",\"demolish\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"demolition\",\"demolish\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"derailment\",\"derail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"derailed\",\"derail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"desolation\",\"desolate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"destroyed\",\"destroy\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"destruction\",\"destroy\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"detonate\",\"detonation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"devastated\",\"devastation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"drowned\",\"drown\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"drowning\",\"drown\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"electrocute\",\"electrocuted\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuated\",\"evacuate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuation\",\"evacuate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"explode\",\"explosion\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"exploded\",\"explosion\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"fatality\",\"fatalities\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"floods\",\"flood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"flooding\",\"flood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bush%20fires\",\"forest%20fire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"forest%20fires\",\"forest%20fire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hailstorm\",\"hail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hazardous\",\"hazard\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacking\",\"hijack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacker\",\"hijack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hostages\",\"hostage\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"injured\",\"injury\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"injures\",\"injury\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"inundated\",\"inundation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"mass%20murderer\",\"mass%20murder\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"obliterated\",\"obliterate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"obliteration\",\"obliterate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"panicking\",\"panic\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"quarantined\",\"quarantine\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rescuers\",\"rescue\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rescued\",\"rescue\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rioting\",\"riot\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"dust%20storm\",\"sandstorm\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"screamed\",\"screams\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"screaming\",\"screams\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"sirens\",\"siren\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bomb\",\"suicide%20bomber\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bombing\",\"suicide%20bomber\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"survived\",\"survive\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"survivors\",\"survive\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"terrorism\",\"terrorist\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"thunderstorm\",\"thunder\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"traumatised\",\"trauma\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"twister\",\"tornado\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"typhoon\",\"hurricane\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"weapons\",\"weapon\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wild%20fires\",\"wildfire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wounded\",\"wounds\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wrecked\",\"wreckage\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wreck\",\"wreckage\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91597d-b14a-44bb-b1ad-05e224142f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = replace_keywords(train_df)\n",
    "test_df = replace_keywords(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bec67a-3124-477e-a143-f458ccea0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_keywords = train_df[\"keyword\"].unique()[1:]\n",
    "kword_resArr = []\n",
    "print(len(uniq_keywords))\n",
    "for kword in uniq_keywords:\n",
    "    kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n",
    "    total_kword = float(len(kword_df))\n",
    "    target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n",
    "    target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n",
    "    kword_prob_df = pd.DataFrame({'keyword':[kword],\n",
    "                                 \"keywordPred0\": [target0_n/total_kword],\n",
    "                                 \"keywordPred1\": [target1_n/total_kword]})\n",
    "    kword_resArr.append(kword_prob_df)\n",
    "predProbGivenKeyWord_df= pd.concat(kword_resArr)\n",
    "predProbGivenKeyWord_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76150c8f-b00d-4451-89c8-730d1382a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"textprob0\"]=predProbGivenText_df.loc[:,0].copy()\n",
    "test_df[\"textprob1\"]=predProbGivenText_df.loc[:,1].copy()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c7ed0-ac9a-46a5-900a-099e2310d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(predProbGivenKeyWord_df, how='left', on=\"keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8dde3-92a4-4891-a48a-4043ec40bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"keywordPred0\"]=test_df[\"keywordPred0\"].fillna(0.5)\n",
    "test_df[\"keywordPred1\"]=test_df[\"keywordPred1\"].fillna(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a8621-bb29-447b-82f0-08a44e3fe9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"pred0\"]=test_df[\"textprob0\"]*test_df[\"keywordPred0\"]\n",
    "test_df[\"pred1\"]=test_df[\"textprob1\"]*test_df[\"keywordPred1\"]\n",
    "test_df[\"target\"]=test_df[\"pred1\"]>test_df[\"pred0\"]\n",
    "test_df[\"target\"] = test_df[\"target\"].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b19018-f8e0-4643-8b5d-669416ee3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df.loc[:,[\"id\",\"target\"]]\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7546d-bebb-4841-b1af-27d2bfeb49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e51e00-066e-4ace-b0b1-4e5bc9e88cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde40ab-a1a1-4001-ac11-0dad04b6068e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109bf9e-088a-4ff0-a05c-daee494dea42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
